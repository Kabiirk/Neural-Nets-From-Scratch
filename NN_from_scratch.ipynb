{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NN_from_scratch.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "hXCD6EPlez-Y",
        "0p7vVzKXxrUE"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXCD6EPlez-Y"
      },
      "source": [
        "# **Dependencies**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QW0-ImUNc2JU"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib\n",
        "import sys"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOOH_RWYe7rI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21faec1a-e282-4701-f55b-8702c809bd9c"
      },
      "source": [
        "print(\"Python: \", sys.version)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Python:  3.6.9 (default, Oct  8 2020, 12:12:24) \n",
            "[GCC 8.4.0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePBlH6yWe_CG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "570cecb6-680c-46f6-c3e1-83a6eb1d4764"
      },
      "source": [
        "inputs = [1, 2, 3, 2.5]\n",
        "\n",
        "weights = [[0.2, 0.8, -0.5, 1.0],\n",
        "           [0.5, -0.91, 0.26, -0.5],\n",
        "           [-0.26, -0.27, 0.17, 0.87]]\n",
        "\n",
        "biases = [2, 3, 0.5]\n",
        "\n",
        "layer_output = []\n",
        "\n",
        "for neuron_weights, neuron_bias in zip(weights, biases):\n",
        "    neuron_output = 0\n",
        "    for neuron_input, weight in zip(inputs, neuron_weights):\n",
        "        neuron_output += neuron_input*weight\n",
        "    neuron_output += neuron_bias\n",
        "    layer_output.append(neuron_output)\n",
        "\n",
        "print(layer_output)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[4.8, 1.21, 2.385]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bxtkVmgsaiY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "32d76218-5f98-4052-9482-f210fc24be2b"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "inputs = [1, 2, 3, 2.5]\n",
        "\n",
        "weights = [[0.2, 0.8, -0.5, 1.0],\n",
        "           [0.5, -0.91, 0.26, -0.5],\n",
        "           [-0.26, -0.27, 0.17, 0.87]]\n",
        "\n",
        "biases = [2, 3, 0.5]\n",
        "\n",
        "#1st elem we pass is how the element is gonna be indexed\n",
        "output = np.dot(weights, inputs) + biases\n",
        "print(output)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[4.8   1.21  2.385]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0p7vVzKXxrUE"
      },
      "source": [
        "# **Batches etc**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qv-tr_SmvhbK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b76cb27c-1421-49de-c9eb-bd3cea0467cd"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "#batches allow us to paralellize operations\n",
        "#batches also help us in generalization\n",
        "\n",
        "inputs = [[1, 2, 3, 2.5],\n",
        "          [2.0, 5.0, -1.0, 2.0],\n",
        "          [-1.5, 2.7, 3.3, -0.8]] #3x4\n",
        "\n",
        "weights = [[0.2, 0.8, -0.5, 1.0],\n",
        "           [0.5, -0.91, 0.26, -0.5],\n",
        "           [-0.26, -0.27, 0.17, 0.87]] #3x4\n",
        "\n",
        "biases = [2, 3, 0.5]\n",
        "\n",
        "#Layer 2\n",
        "weights2 = [[0.1, -0.14, 0.5],\n",
        "           [-0.5, 0.12, 0.33],\n",
        "           [-0.44, 0.73, -0.13]] #3x4\n",
        "\n",
        "biases2 = [-1, 2, -0.5]\n",
        "\n",
        "#output = np.dot(weights, inputs) + biases\n",
        "#gives issue, we need to change shape by transposition\n",
        "\n",
        "#Layers\n",
        "layer1_outputs = np.dot(inputs, np.array(weights).T) + biases\n",
        "\n",
        "layer2_outputs = np.dot(layer1_outputs, np.array(weights2).T) + biases2\n",
        "\n",
        "print(layer2_outputs)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 0.5031   0.53225 -2.03875]\n",
            " [ 0.2434  -2.6012  -5.7633 ]\n",
            " [-0.99314  1.4297  -0.35655]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajNa3HPXx1nD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b63e4cf5-3652-4d81-9439-4c55ee2d46f2"
      },
      "source": [
        "import numpy as np\n",
        "#same code as above but better\n",
        "np.random.seed(0)\n",
        "\n",
        "X = [[1, 2, 3, 2.5],\n",
        "          [2.0, 5.0, -1.0, 2.0],\n",
        "          [-1.5, 2.7, 3.3, -0.8]]\n",
        "\n",
        "class Layer_Dense:\n",
        "    def __init__(self, n_inputs, n_neurons):\n",
        "        self.weights = 0.1*np.random.randn(n_inputs, n_neurons)\n",
        "        self.biases = np.zeros((1, n_neurons))\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        self.output = np.dot(inputs, self.weights) + self.biases\n",
        "\n",
        "layer1 = Layer_Dense(4,5)#4 = Features, 5 = No. of Neurons\n",
        "layer2 = Layer_Dense(5,2) # L1 has 5 op so l2 shld have 5 ip\n",
        "\n",
        "print(\"Layer 1\")\n",
        "layer1.forward(X)\n",
        "print(layer1.output)\n",
        "print(\"\\n\")\n",
        "print(\"Layer 2\")\n",
        "layer2.forward(layer1.output)\n",
        "print(layer2.output)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Layer 1\n",
            "[[ 0.10758131  1.03983522  0.24462411  0.31821498  0.18851053]\n",
            " [-0.08349796  0.70846411  0.00293357  0.44701525  0.36360538]\n",
            " [-0.50763245  0.55688422  0.07987797 -0.34889573  0.04553042]]\n",
            "\n",
            "\n",
            "Layer 2\n",
            "[[ 0.148296   -0.08397602]\n",
            " [ 0.14100315 -0.01340469]\n",
            " [ 0.20124979 -0.07290616]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cX6sClj89K72"
      },
      "source": [
        "# **Activation Functions**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5Ta5DnZ-etm"
      },
      "source": [
        "#### ReLU Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UFaHZLpz9PaG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2167f3f9-e787-481a-8914-9c523bf665fe"
      },
      "source": [
        "X = [[1, 2, 3, 2.5],\n",
        "    [2.0, 5.0, -1.0, 2.0],\n",
        "    [-1.5, 2.7, 3.3, -0.8]]\n",
        "\n",
        "inputs = [0, 2, -1, 3.3, -2.7, 1.1, 2.2, -100]\n",
        "output = []\n",
        "\n",
        "for i in inputs:\n",
        "    '''\n",
        "    do the same thing as uncommented code below but better !!! LMAOOOOOO\n",
        "    if(i>0):\n",
        "        output.append(i)\n",
        "    else:\n",
        "        output.append(0)\n",
        "    '''\n",
        "    output.append(max(0, i))\n",
        "\n",
        "print(output)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0, 2, 0, 3.3, 0, 1.1, 2.2, 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vezl_K-GGt3c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3340f118-e061-4655-c96f-d4025cba7cb2"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        " \n",
        "#same code as above but better\n",
        "np.random.seed(0)\n",
        "\n",
        "#Create Dataset\n",
        "def create_data(points, classes):\n",
        "    X = np.zeros((points*classes, 2))\n",
        "    y = np.zeros(points*classes, dtype='uint8')\n",
        "    for class_number in range(classes):\n",
        "        ix = range(points*class_number, points*(class_number+1))\n",
        "        r = np.linspace(0.0, 1, points)\n",
        "        t = np.linspace(class_number*4, (class_number+1)*4, points) + np.random.randn(points)*0.2\n",
        "        X[ix] = np.c_[r*np.sin(t*2.5), r*np.cos(t*2.5)]\n",
        "        y[ix] = class_number\n",
        "    return X, y\n",
        "\n",
        "class Layer_Dense:\n",
        "    def __init__(self, n_inputs, n_neurons):\n",
        "        self.weights = 0.1*np.random.randn(n_inputs, n_neurons)\n",
        "        self.biases = np.zeros((1, n_neurons))\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        self.output = np.dot(inputs, self.weights) + self.biases\n",
        "\n",
        "class Activation_ReLU:\n",
        "    def forward(self, inputs):\n",
        "        self.output = np.maximum(0, inputs)\n",
        "\n",
        "X, y = create_data(100, 3)\n",
        "\n",
        "'''\n",
        "plt.scatter(X[:,0], X[:,1])\n",
        "plt.show()\n",
        "\n",
        "plt.scatter(X[:,0], X[:,1], c=y, cmap=\"brg\")\n",
        "plt.show()\n",
        "'''\n",
        "\n",
        "layer1 = Layer_Dense(2,5)\n",
        "#creation of activation object + using in layers\n",
        "activation1 = Activation_ReLU()\n",
        "\n",
        "layer1.forward(X)\n",
        "activation1.forward(layer1.output)\n",
        "print(activation1.output)\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.00000000e+00 0.00000000e+00 0.00000000e+00 0.00000000e+00\n",
            "  0.00000000e+00]\n",
            " [0.00000000e+00 0.00000000e+00 0.00000000e+00 4.65504526e-04\n",
            "  4.56845892e-05]\n",
            " [0.00000000e+00 5.93467943e-05 0.00000000e+00 2.03573189e-04\n",
            "  6.10024276e-04]\n",
            " ...\n",
            " [1.13291515e-01 0.00000000e+00 0.00000000e+00 8.11079627e-02\n",
            "  0.00000000e+00]\n",
            " [1.34588354e-01 0.00000000e+00 3.09493973e-02 5.66337522e-02\n",
            "  0.00000000e+00]\n",
            " [1.07817915e-01 0.00000000e+00 0.00000000e+00 8.72561871e-02\n",
            "  0.00000000e+00]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--cd97gbO14O"
      },
      "source": [
        "#### Softmax Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vV-Kw2yGO9-D"
      },
      "source": [
        "# ReLU Activatio function too inaccurate for backprop\r\n",
        "# Since it clips all negative values to 0\r\n",
        "# So a network has no idea on how \"Wrong\" or \"Right\" an output is \r\n",
        "\r\n",
        "# Softmax :\r\n",
        "# \r\n",
        "# y = exp(x)\r\n",
        "# e ~ 2.7182\r\n",
        "\r\n",
        "# Steps:\r\n",
        "# imput -> exponentiation -> normalization -> output\r\n",
        "#          |______________________________|\r\n",
        "#                         |\r\n",
        "#                      Softmax\r\n",
        "\r\n",
        "# Example:\r\n",
        "#                                                  exp(1)                    exp(2)                    exp(3)\r\n",
        "# [1,2,3] -> [exp(1), exp(2), exp(3)] -> [------------------------, ------------------------, ------------------------ ] -> [0.09, 0.24, 0.67]\r\n",
        "#                                         exp(1) + exp(2) + exp(3)  exp(1) + exp(2) + exp(3)  exp(1) + exp(2) + exp(3)\r\n",
        "\r\n",
        "# Therefore sofmatx is:\r\n",
        "#          exp(z(i,j))\r\n",
        "# S(i,j) = -------------\r\n",
        "#          ∑ exp(z(i,j))\r\n",
        "#          (l=1 -> L)\r\n",
        "\r\n",
        "import math"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vU4KKFc7Qd5k",
        "outputId": "dce1ceea-4b4e-4164-e532-e072da79e359"
      },
      "source": [
        "layer_output = [4.8, 1.21, 2.385]\r\n",
        "\r\n",
        "# 1. Exponentiation\r\n",
        "E = math.e # ~ 2.7182\r\n",
        "\r\n",
        "exp_values = []\r\n",
        "\r\n",
        "for output in layer_output:\r\n",
        "    exp_values.append(E**output)\r\n",
        "\r\n",
        "print(exp_values)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[121.51041751873483, 3.353484652549023, 10.859062664920513]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7S-25PkhQzQN",
        "outputId": "6708cf84-fa9c-45f3-bed0-cf62fe641820"
      },
      "source": [
        "# 2. Normalization\r\n",
        "\r\n",
        "# Here :\r\n",
        "#                      (Single neuron Value)\r\n",
        "# (normalized value) = --------------------------\r\n",
        "#                      (sum of all neuron values)\r\n",
        "\r\n",
        "norm_base = sum(exp_values)\r\n",
        "norm_values = []\r\n",
        "\r\n",
        "for value in exp_values:\r\n",
        "    norm_values.append(value/norm_base)\r\n",
        "\r\n",
        "print(norm_values)\r\n",
        "print(sum(norm_values)) # should be ~ 1"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.8952826639572619, 0.024708306782099374, 0.0800090292606387]\n",
            "0.9999999999999999\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j7UgQ6y5Ron8",
        "outputId": "78c1735b-066d-4b93-c5c0-83e246aef87b"
      },
      "source": [
        "# Same code with numpy \r\n",
        "# Numpy makes it shorter\r\n",
        "\r\n",
        "# 1. Exponentiation\r\n",
        "exp_values = np.exp(layer_output) # Applies to the whole array\r\n",
        "print(exp_values)\r\n",
        "\r\n",
        "# 2. Normalization\r\n",
        "norm_values = exp_values/np.sum(exp_values)\r\n",
        "print(norm_values)\r\n",
        "print(sum(norm_values)) # should be ~ 1"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[121.51041752   3.35348465  10.85906266]\n",
            "[0.89528266 0.02470831 0.08000903]\n",
            "0.9999999999999999\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98nzIsAjSfri",
        "outputId": "30881554-9385-4a1c-bf7c-f9066696f54b"
      },
      "source": [
        "# Making output as a Batch\r\n",
        "layer_outputs = [[4.8, 1.21, 2.385],\r\n",
        "                 [8.9, -1.81, 0.2],\r\n",
        "                 [1.41, 1.051, 0.026]]\r\n",
        "\r\n",
        "exp_values = np.exp(layer_outputs)\r\n",
        "\r\n",
        "# print(np.sum(layer_outputs, axis=1, keepdims=True)) \r\n",
        "# axis=0 -> sum of columns\r\n",
        "# axis=1 -> sum of rows\r\n",
        "# keepdims=True -> retains original shape of original \"Dimenesion\" or orientation\r\n",
        "#\r\n",
        "# The output of this comand is:\r\n",
        "# [[8.395]\r\n",
        "#  [7.29 ]\r\n",
        "#  [2.487]]\r\n",
        "\r\n",
        "# Therefore to normalize properly we do:\r\n",
        "norm_values = exp_values/np.sum(exp_values, axis=1, keepdims=True)\r\n",
        "print(norm_values)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[8.95282664e-01 2.47083068e-02 8.00090293e-02]\n",
            " [9.99811129e-01 2.23163963e-05 1.66554348e-04]\n",
            " [5.13097164e-01 3.58333899e-01 1.28568936e-01]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H9hdXY9lWCcg",
        "outputId": "a1ff4f40-d762-4d24-e61f-9dae8edb5517"
      },
      "source": [
        "# Combining it all together\r\n",
        "\r\n",
        "# ===== Some Code reused from ReLU CodeBlock =====\r\n",
        "\r\n",
        "# Create Dataset\r\n",
        "def create_data(points, classes):\r\n",
        "    X = np.zeros((points*classes, 2))\r\n",
        "    y = np.zeros(points*classes, dtype='uint8')\r\n",
        "    for class_number in range(classes):\r\n",
        "        ix = range(points*class_number, points*(class_number+1))\r\n",
        "        r = np.linspace(0.0, 1, points)\r\n",
        "        t = np.linspace(class_number*4, (class_number+1)*4, points) + np.random.randn(points)*0.2\r\n",
        "        X[ix] = np.c_[r*np.sin(t*2.5), r*np.cos(t*2.5)]\r\n",
        "        y[ix] = class_number\r\n",
        "    return X, y\r\n",
        "\r\n",
        "class Layer_Dense:\r\n",
        "    def __init__(self, n_inputs, n_neurons):\r\n",
        "        self.weights = 0.1*np.random.randn(n_inputs, n_neurons)\r\n",
        "        self.biases = np.zeros((1, n_neurons))\r\n",
        "\r\n",
        "    def forward(self, inputs):\r\n",
        "        self.output = np.dot(inputs, self.weights) + self.biases\r\n",
        "\r\n",
        "class Activation_ReLU:\r\n",
        "    def forward(self, inputs):\r\n",
        "        self.output = np.maximum(0, inputs)\r\n",
        "\r\n",
        "# ===== End of code used from ReLU Codeblock =====\r\n",
        "\r\n",
        "class Activation_Softmax:\r\n",
        "    def forward(self, inputs):\r\n",
        "        exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\r\n",
        "        # As x increases in exp(x), values start becoming very large\r\n",
        "        # this can give us an overflow error\r\n",
        "        # therefore to manage these values,\r\n",
        "        # we take all values of output layer before exponentiation\r\n",
        "        # then we subtract the largest value in that layer from everi value in that layer\r\n",
        "        # this means that the largest value now becomes 0; and our range of possiblities\r\n",
        "        # are now ranging from 0 to 1, after exponentiation\r\n",
        "        # Our output is still the same i.e. unaffected\r\n",
        "        # Doing this protects us from an overfloe error\r\n",
        "        probablities = exp_values/np.sum(exp_values, axis=1, keepdims=True)\r\n",
        "        self.output = probablities\r\n",
        "\r\n",
        "X, y = create_data(100, 3)\r\n",
        "\r\n",
        "\r\n",
        "dense1 = Layer_Dense(2,3)\r\n",
        "activation1 = Activation_ReLU()\r\n",
        "dense2 = Layer_Dense(3,3)\r\n",
        "# input should be same as output of previous layer i.e. 3\r\n",
        "# This layer also has 3 outputs\r\n",
        "activation2 = Activation_Softmax()\r\n",
        "\r\n",
        "# Running the Neural Net\r\n",
        "dense1.forward(X)\r\n",
        "activation1.forward(dense1.output)\r\n",
        "dense2.forward(activation1.output)\r\n",
        "activation2.forward(dense2.output)\r\n",
        "\r\n",
        "print(activation2.output[:5]) # Print first 5 of multiple output"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.33333333 0.33333333 0.33333333]\n",
            " [0.33335656 0.3333228  0.33332064]\n",
            " [0.33337937 0.33331247 0.33330816]\n",
            " [0.33340728 0.33329962 0.3332931 ]\n",
            " [0.33343321 0.33328771 0.33327908]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}